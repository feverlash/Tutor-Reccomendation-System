{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel, AutoTokenizer\nfrom transformers import DataCollatorWithPadding\nfrom datasets import load_dataset, concatenate_datasets\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport string\nimport time\nfrom tqdm import tqdm\nfrom datetime import timedelta","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T10:08:31.560857Z","iopub.execute_input":"2024-06-12T10:08:31.561272Z","iopub.status.idle":"2024-06-12T10:09:01.967000Z","shell.execute_reply.started":"2024-06-12T10:08:31.561238Z","shell.execute_reply":"2024-06-12T10:09:01.966094Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-12 10:08:35.131452: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-12 10:08:35.131564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-12 10:08:35.402708: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. Load the STSB dataset: https://huggingface.co/datasets/sentence-transformers/stsb\ntrain_dataset = load_dataset(\"feverlash/stsb-indo-dataset\", split=\"train\")\neval_dataset = load_dataset(\"feverlash/stsb-indo-dataset\", split=\"validation\")\ntest_dataset = load_dataset(\"feverlash/stsb-indo-dataset\", split=\"test\")\n\ntrain_data = train_dataset.map(lambda example: {'sentence1': example['sentence1'], 'sentence2': example['sentence2'], 'score': example['score'] / 5.0})\neval_data = eval_dataset.map(lambda example: {'sentence1': example['sentence1'], 'sentence2': example['sentence2'], 'score': example['score'] / 5.0})\ntest_data = test_dataset.map(lambda example: {'sentence1': example['sentence1'], 'sentence2': example['sentence2'], 'score': example['score'] / 5.0})\n\ntokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n\nmax_length = 128\nbatch_size = 32\neval_batch_size = 32\n\n# 3. Tokenize the dataset\ndef process(first_token:str, second_token:str):\n    inputs = tokenizer([first_token, second_token],\n                            max_length=max_length,\n                            truncation=True,\n                            padding=\"max_length\",\n                            return_tensors='tf')\n    return inputs\n\ndef tokenize_function(examples):\n    first_sent = examples['sentence1']\n    second_sent = examples['sentence2']\n    tokenized_sentences = process(first_sent, second_sent)\n    return tokenized_sentences\n\ntrain_ds = train_data.map(tokenize_function, batched=False)\neval_ds = eval_data.map(tokenize_function, batched=False)\ntest_ds = test_data.map(tokenize_function, batched=False)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 4. Prepare dataset for TensorFlow\ntrain_dataset = train_ds.to_tf_dataset(\n    columns=['input_ids', 'attention_mask'],\n    label_cols=['score'],\n    shuffle=True,\n    batch_size=batch_size\n)\neval_dataset = eval_ds.to_tf_dataset(\n    columns=['input_ids', 'attention_mask'],\n    label_cols=['score'],\n    shuffle=False,\n    batch_size=eval_batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:01.968929Z","iopub.execute_input":"2024-06-12T10:09:01.970259Z","iopub.status.idle":"2024-06-12T10:09:06.385409Z","shell.execute_reply.started":"2024-06-12T10:09:01.970216Z","shell.execute_reply":"2024-06-12T10:09:06.384575Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\nOld behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \nNew behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def collate_fn(texts):\n\n    input_ids = texts['input_ids']\n    attention_masks = texts['attention_mask']\n\n    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n                for input_id, attention_mask in zip(input_ids, attention_masks)]\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:06.386580Z","iopub.execute_input":"2024-06-12T10:09:06.386893Z","iopub.status.idle":"2024-06-12T10:09:06.392386Z","shell.execute_reply.started":"2024-06-12T10:09:06.386867Z","shell.execute_reply":"2024-06-12T10:09:06.391322Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# class CosineSimilarityLoss(tf.keras.losses.Loss):\n\n#     def __init__(self, loss_fn=tf.keras.losses.MeanSquaredError(name='mean_squared_error'), transform_fn=tf.identity, name=\"cosine_similarity_loss\"):\n#         super(CosineSimilarityLoss, self).__init__(name=name)\n#         self.loss_fn = loss_fn\n#         self.transform_fn = transform_fn\n\n#     def call(self, y_true, y_pred):\n#         # y_pred is expected to be a tuple of two embeddings\n#         emb_1 = tf.stack([pred[0] for pred in y_pred])\n#         emb_2 = tf.stack([pred[1] for pred in y_pred])\n#         # Compute cosine similarity\n#         cos_similarity = abs(tf.keras.losses.cosine_similarity(emb_1, emb_2, axis=1))\n#         # Transform the cosine similarity\n#         transformed_similarity = self.transform_fn(cos_similarity)\n#         # Compute the loss\n#         return self.loss_fn(tf.squeeze(y_true), transformed_similarity)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:06.394566Z","iopub.execute_input":"2024-06-12T10:09:06.394916Z","iopub.status.idle":"2024-06-12T10:09:06.425619Z","shell.execute_reply.started":"2024-06-12T10:09:06.394887Z","shell.execute_reply":"2024-06-12T10:09:06.424348Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class BertForSTS(tf.keras.Model):\n\n    def __init__(self):\n        super(BertForSTS, self).__init__()\n        self.bert = TFBertModel.from_pretrained('indobenchmark/indobert-base-p1')\n\n    def call(self, input_data, training=False):\n#         # Tokenize the input\n#         inputs = self.tokenizer(input_data, return_tensors='tf', max_length=128, truncation=True, padding=True)\n        outputs = self.bert(input_data)\n        cls_output = outputs.last_hidden_state  # CLS token\n        attention = input_data['attention_mask']\n        mask = tf.cast(tf.expand_dims(attention, -1), dtype=tf.float32)\n        masked_embeddings = cls_output * mask\n        summed = tf.reduce_sum(masked_embeddings, axis=1)\n        counts = tf.clip_by_value(tf.reduce_sum(mask, axis=1), clip_value_min=1e-9, clip_value_max=tf.float32.max)\n        mean_pooled = summed / counts\n        return mean_pooled","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:06.427018Z","iopub.execute_input":"2024-06-12T10:09:06.427331Z","iopub.status.idle":"2024-06-12T10:09:06.729313Z","shell.execute_reply.started":"2024-06-12T10:09:06.427304Z","shell.execute_reply":"2024-06-12T10:09:06.728486Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = BertForSTS()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:06.730442Z","iopub.execute_input":"2024-06-12T10:09:06.730753Z","iopub.status.idle":"2024-06-12T10:09:10.414268Z","shell.execute_reply.started":"2024-06-12T10:09:06.730726Z","shell.execute_reply":"2024-06-12T10:09:10.413430Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"class CosineSimilarityLoss(tf.keras.losses.Loss):\n    def __init__(self, loss_fn=tf.keras.losses.MeanSquaredError(name='mean_squared_error'), transform_fn=tf.identity, name=\"cosine_similarity_loss\"):\n        super(CosineSimilarityLoss, self).__init__(name=name)\n        self.loss_fn = loss_fn\n        self.transform_fn = transform_fn\n\n    def call(self, y_true, y_pred):\n        # y_pred is expected to be a tuple of two embeddings\n        emb_1 = tf.stack([pred[0] for pred in y_pred])\n        emb_2 = tf.stack([pred[1] for pred in y_pred])\n        # Compute cosine similarity\n        cos_similarity = abs(tf.keras.losses.cosine_similarity(emb_1, emb_2, axis=1))\n        # Transform the cosine similarity\n        transformed_similarity = self.transform_fn(cos_similarity)\n        # Compute the loss\n        return self.loss_fn(tf.squeeze(y_true), transformed_similarity)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:10.415719Z","iopub.execute_input":"2024-06-12T10:09:10.416101Z","iopub.status.idle":"2024-06-12T10:09:10.424809Z","shell.execute_reply.started":"2024-06-12T10:09:10.416063Z","shell.execute_reply":"2024-06-12T10:09:10.423694Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# train_loss = tf.keras.metrics.Mean(name='train_loss')\n# total_train_loss = 0\n# for input, labels in tqdm(train_dataset.take(2)):\n#     input['input_ids'] = input['input_ids']\n#     input['attention_mask'] = input['attention_mask']\n#     print(np.shape(input['input_ids']))\n#     print(labels)\n#     input = collate_fn(input)\n#     print(np.shape(input))\n#     embeddings = [model(feature) for feature in input]\n#     print(\"embbedding:\", np.shape(embeddings))\n#     loss_fn = CosineSimilarityLoss()\n#     emb_1 = tf.stack([pred[0] for pred in embeddings])\n#     emb_2 = tf.stack([pred[1] for pred in embeddings])\n#     print(\"embbedding 1 :\", emb_1)\n#     print(\"embbedding 2 :\", emb_2)\n#     cos_similarity = abs(tf.keras.losses.cosine_similarity(emb_1, emb_2, axis=1))\n#     print(cos_similarity)\n#     loss  = loss_fn(labels, embeddings)\n#     total_train_loss += loss\n#     print(\"LOSS : \",loss)\n#     train_loss(loss)\n# avg_loss = total_train_loss/len(train_dataset.take(2))\n# avg_train_loss = train_loss.result()\n# print(avg_train_loss)\n# print(avg_loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:10.426124Z","iopub.execute_input":"2024-06-12T10:09:10.426437Z","iopub.status.idle":"2024-06-12T10:09:10.437431Z","shell.execute_reply.started":"2024-06-12T10:09:10.426410Z","shell.execute_reply":"2024-06-12T10:09:10.436687Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nsteps_per_epoch = len(train_dataset) // batch_size\nnum_train_steps = steps_per_epoch * epochs\nprint(len(train_dataset),steps_per_epoch, num_train_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:10.438594Z","iopub.execute_input":"2024-06-12T10:09:10.439309Z","iopub.status.idle":"2024-06-12T10:09:10.457514Z","shell.execute_reply.started":"2024-06-12T10:09:10.439282Z","shell.execute_reply":"2024-06-12T10:09:10.456620Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"180 5 25\n","output_type":"stream"}]},{"cell_type":"code","source":"def format_time(elapsed):\n    return str(timedelta(seconds=int(round((elapsed)))))\n\n@tf.function\ndef train_step(model, input, labels, optimizer, loss_fn):\n    with tf.GradientTape() as tape:\n        embeddings = [model(feature, training=True) for feature in input]\n        loss = loss_fn(labels, embeddings)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n@tf.function\ndef val_step(model, input, labels, optimizer, loss_fn):\n    embeddings = [model(feature, training=True) for feature in input]\n    loss = loss_fn(labels, embeddings)\n    return loss\n\ndef train(model, train_dataset, eval_dataset, epochs, batch_size):\n    seed_val = 42\n\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    tf.random.set_seed(seed_val)\n\n    # Prepare optimizer, learning rate scheduler\n    steps_per_epoch = len(train_dataset) // batch_size\n    num_train_steps = steps_per_epoch * epochs\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    loss_fn = CosineSimilarityLoss()\n\n    # Metrics to track loss\n    # train_loss = tf.keras.metrics.Mean(name='train_loss')\n    # val_loss = tf.keras.metrics.Mean(name='val_loss')\n\n    # Training and validation loops\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(epochs):\n        print(f\"======== Epoch {epoch_i + 1} / {epochs} ========\")\n        print('Training...')\n\n        t0 = time.time()\n        total_train_loss = 0\n\n        for input, labels in tqdm(train_dataset):\n            input['input_ids'] = input['input_ids']\n            input['attention_mask'] = input['attention_mask']\n            input = collate_fn(input)\n            loss = train_step(model, input, labels, optimizer, loss_fn)\n            total_train_loss += loss\n\n        avg_train_loss = total_train_loss/len(train_dataset)\n        training_time = format_time(time.time() - t0)\n\n        print(f\"  Average training loss: {avg_train_loss:.5f}\")\n        print(f\"  Training epoch took: {training_time}\")\n\n        # Validation loop\n        print(\"Running Validation...\")\n        t0 = time.time()\n        \n        total_val_loss = 0\n\n        for input, labels in tqdm(eval_dataset):\n            input['input_ids'] = input['input_ids']\n            input['attention_mask'] = input['attention_mask']\n            input = collate_fn(input)\n            loss = val_step(model, input, labels, optimizer, loss_fn)\n            total_val_loss += loss\n\n        avg_val_loss = total_val_loss/len(eval_dataset)\n        validation_time = format_time(time.time() - t0)\n\n        print(f\"  Validation Loss: {avg_val_loss:.5f}\")\n        print(f\"  Validation took: {validation_time}\")\n\n        # Record all statistics from this epoch\n        training_stats.append(\n            {\n                'epoch': epoch_i + 1,\n                'Training Loss': avg_train_loss.numpy(),\n                'Valid. Loss': avg_val_loss.numpy(),\n                'Training Time': training_time,\n                'Validation Time': validation_time\n            }\n        )\n\n    print(\"Training complete!\")\n    print(f\"Total training took {format_time(time.time() - total_t0)} (h:mm:ss)\")\n\n    return model, training_stats","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:09:10.460530Z","iopub.execute_input":"2024-06-12T10:09:10.460912Z","iopub.status.idle":"2024-06-12T10:09:10.477654Z","shell.execute_reply.started":"2024-06-12T10:09:10.460886Z","shell.execute_reply":"2024-06-12T10:09:10.476798Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model, training_stats = train(model, train_dataset, eval_dataset, epochs, batch_size)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-12T10:09:10.478990Z","iopub.execute_input":"2024-06-12T10:09:10.479600Z","iopub.status.idle":"2024-06-12T11:09:29.491462Z","shell.execute_reply.started":"2024-06-12T10:09:10.479564Z","shell.execute_reply":"2024-06-12T11:09:29.490431Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"======== Epoch 1 / 5 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/180 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718187824.978833   11789 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100%|██████████| 180/180 [25:21<00:00,  8.45s/it]  \n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.03458\n  Training epoch took: 0:25:22\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [10:03<00:00, 12.84s/it]  \n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.02737\n  Validation took: 0:10:03\n======== Epoch 2 / 5 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 180/180 [05:43<00:00,  1.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.01585\n  Training epoch took: 0:05:44\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.02619\n  Validation took: 0:00:32\n======== Epoch 3 / 5 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 180/180 [05:42<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.00899\n  Training epoch took: 0:05:42\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.02768\n  Validation took: 0:00:32\n======== Epoch 4 / 5 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 180/180 [05:40<00:00,  1.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.00646\n  Training epoch took: 0:05:40\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:31<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.02838\n  Validation took: 0:00:32\n======== Epoch 5 / 5 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 180/180 [05:40<00:00,  1.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"  Average training loss: 0.00529\n  Training epoch took: 0:05:40\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:31<00:00,  1.48it/s]","output_type":"stream"},{"name":"stdout","text":"  Validation Loss: 0.02838\n  Validation took: 0:00:32\nTraining complete!\nTotal training took 1:00:19 (h:mm:ss)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n# Create a DataFrame from our training statistics\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table\ndf_stats","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:09:29.502317Z","iopub.execute_input":"2024-06-12T11:09:29.502614Z","iopub.status.idle":"2024-06-12T11:09:29.566887Z","shell.execute_reply.started":"2024-06-12T11:09:29.502588Z","shell.execute_reply":"2024-06-12T11:09:29.565985Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       Training Loss  Valid. Loss Training Time Validation Time\nepoch                                                          \n1           0.034582     0.027372       0:25:22         0:10:03\n2           0.015855     0.026193       0:05:44         0:00:32\n3           0.008994     0.027679       0:05:42         0:00:32\n4           0.006458     0.028383       0:05:40         0:00:32\n5           0.005293     0.028379       0:05:40         0:00:32","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.034582</td>\n      <td>0.027372</td>\n      <td>0:25:22</td>\n      <td>0:10:03</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.015855</td>\n      <td>0.026193</td>\n      <td>0:05:44</td>\n      <td>0:00:32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.008994</td>\n      <td>0.027679</td>\n      <td>0:05:42</td>\n      <td>0:00:32</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.006458</td>\n      <td>0.028383</td>\n      <td>0:05:40</td>\n      <td>0:00:32</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.005293</td>\n      <td>0.028379</td>\n      <td>0:05:40</td>\n      <td>0:00:32</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model_save_path = \"./bert_sts_model/model\"\nmodel.save(\"my_model.h5\")\nprint(f\"Model saved to {model_save_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:16:40.717991Z","iopub.execute_input":"2024-06-12T11:16:40.718813Z","iopub.status.idle":"2024-06-12T11:16:40.816150Z","shell.execute_reply.started":"2024-06-12T11:16:40.718774Z","shell.execute_reply":"2024-06-12T11:16:40.814701Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./bert_sts_model/model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras/src/saving/legacy/save.py:152\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m     save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile))\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m saving_utils\u001b[38;5;241m.\u001b[39mis_hdf5_filepath(filepath)\n\u001b[1;32m    147\u001b[0m ):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    150\u001b[0m         model, sequential\u001b[38;5;241m.\u001b[39mSequential\n\u001b[1;32m    151\u001b[0m     ):\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclassed models, because such models are defined via the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody of a Python method, which isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt safely serializable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider saving to the Tensorflow SavedModel format (by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) or using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    159\u001b[0m         )\n\u001b[1;32m    160\u001b[0m     hdf5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    161\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."],"ename":"NotImplementedError","evalue":"Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.","output_type":"error"}]},{"cell_type":"code","source":"model.save_weights('./checkpoints/my_checkpoint')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:17:48.571536Z","iopub.execute_input":"2024-06-12T11:17:48.572483Z","iopub.status.idle":"2024-06-12T11:17:49.669877Z","shell.execute_reply.started":"2024-06-12T11:17:48.572446Z","shell.execute_reply":"2024-06-12T11:17:49.668683Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!zip \"./checkpoints/my_checkpoint.zip\"","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:21:34.485544Z","iopub.execute_input":"2024-06-12T11:21:34.485994Z","iopub.status.idle":"2024-06-12T11:21:35.770095Z","shell.execute_reply.started":"2024-06-12T11:21:34.485956Z","shell.execute_reply":"2024-06-12T11:21:35.768976Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\nzip error: Nothing to do! (./checkpoints/my_checkpoint.zip)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = test_ds.to_tf_dataset(\n    columns=['input_ids', 'attention_mask'],\n    label_cols=['score'],\n    shuffle=True,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:11:13.460322Z","iopub.execute_input":"2024-06-12T11:11:13.460966Z","iopub.status.idle":"2024-06-12T11:11:13.643771Z","shell.execute_reply.started":"2024-06-12T11:11:13.460935Z","shell.execute_reply":"2024-06-12T11:11:13.642704Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\nOld behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \nNew behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenizing(sentence_pair):\n    input = tokenizer(sentence_pair, padding=True, truncation=True, max_length=max_length, return_tensors=\"tf\")\n    input['input_ids'] = input['input_ids']\n    input['attention_mask'] = input['attention_mask']\n    input['token_type_ids'] = input['token_type_ids']\n    return input","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:11:13.644973Z","iopub.execute_input":"2024-06-12T11:11:13.645226Z","iopub.status.idle":"2024-06-12T11:11:13.650528Z","shell.execute_reply.started":"2024-06-12T11:11:13.645202Z","shell.execute_reply":"2024-06-12T11:11:13.649593Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def predict_similarity(sentence_pair):\n    input = tokenizing(sentence_pair)\n\n    # Ensure input is in the correct format\n    input_ids = tf.convert_to_tensor(input['input_ids'])\n    attention_mask = tf.convert_to_tensor(input['attention_mask'])\n    token_type_ids = tf.convert_to_tensor(input['token_type_ids'])\n\n    embed = model((input_ids, attention_mask), training=False)\n    similiarity = tf.keras.metrics.CosineSimilarity()\n    sim = abs(tf.keras.losses.cosine_similarity(embed[0], embed[1], axis=0))\n    return sim, embed","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:11:13.651573Z","iopub.execute_input":"2024-06-12T11:11:13.651899Z","iopub.status.idle":"2024-06-12T11:11:13.666385Z","shell.execute_reply.started":"2024-06-12T11:11:13.651873Z","shell.execute_reply":"2024-06-12T11:11:13.665682Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Prepare the data\nskor = [i['score'] for i in test_data]\nfirst_sent = [i['sentence1'] for i in test_data]\nsecond_sent = [i['sentence2'] for i in test_data]\nfull_text = [[str(x), str(y)] for x,y in zip(first_sent, second_sent)]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:11:13.667344Z","iopub.execute_input":"2024-06-12T11:11:13.667582Z","iopub.status.idle":"2024-06-12T11:11:13.868272Z","shell.execute_reply.started":"2024-06-12T11:11:13.667560Z","shell.execute_reply":"2024-06-12T11:11:13.867560Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"example_1 = full_text[100]\nprint(f\"Sentence 1: {example_1[0]}\")\nprint(f\"Sentence 2: {example_1[1]}\")\nsim, embed = predict_similarity(example_1)\nprint(f\"Predicted similarity score: {round(sim.numpy(),2)}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:11:13.870119Z","iopub.execute_input":"2024-06-12T11:11:13.870392Z","iopub.status.idle":"2024-06-12T11:11:14.523816Z","shell.execute_reply.started":"2024-06-12T11:11:13.870368Z","shell.execute_reply":"2024-06-12T11:11:14.522511Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Sentence 1: Seekor kucing sedang berjalan di sekitar rumah.\nSentence 2: Seorang wanita sedang mengupas kentang.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_1[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m sim, embed \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted similarity score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(sim\u001b[38;5;241m.\u001b[39mnumpy(),\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mpredict_similarity\u001b[0;34m(sentence_pair)\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m similiarity \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCosineSimilarity()\n\u001b[1;32m     11\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcosine_similarity(embed[\u001b[38;5;241m0\u001b[39m], embed[\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mBertForSTS.call\u001b[0;34m(self, input_data, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_data)\n\u001b[1;32m     11\u001b[0m cls_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# CLS token\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mexpand_dims(attention, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     14\u001b[0m masked_embeddings \u001b[38;5;241m=\u001b[39m cls_output \u001b[38;5;241m*\u001b[39m mask\n","\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer 'bert_for_sts' (type BertForSTS).\n\ntuple indices must be integers or slices, not str\n\nCall arguments received by layer 'bert_for_sts' (type BertForSTS):\n  • input_data=('tf.Tensor(shape=(2, 10), dtype=int32)', 'tf.Tensor(shape=(2, 10), dtype=int32)')\n  • training=False"],"ename":"TypeError","evalue":"Exception encountered when calling layer 'bert_for_sts' (type BertForSTS).\n\ntuple indices must be integers or slices, not str\n\nCall arguments received by layer 'bert_for_sts' (type BertForSTS):\n  • input_data=('tf.Tensor(shape=(2, 10), dtype=int32)', 'tf.Tensor(shape=(2, 10), dtype=int32)')\n  • training=False","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}